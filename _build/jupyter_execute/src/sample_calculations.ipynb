{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Calculations\n",
    "\n",
    "Five random points at two different pre-selected timestamps are used to test whether the CDO software accurately calculated pre-processing steps as well as anomalies, means and variances.\n",
    "\n",
    "The bandpass calculation was not recreated as the process is computationally expensive -- CDO took over two days of CPU time, even with various optimisation procedures.\n",
    "\n",
    "Deseasonlisation and long-term averages show no discrepancies between the sample calculations and CDO. Neither do the variance and mean height anomalies -- use of CDO module is considered valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_data(ds):\n",
    "    arr_size = ds.lat.size * ds.lon.size\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    points = rng.choice(arr_size, 5)\n",
    "\n",
    "    points_indices = np.array(np.unravel_index(points, (ds.lon.size, ds.lat.size))).transpose()\n",
    "    points_coords = points_indices - np.array([180, 90])\n",
    "\n",
    "    # Ensure all surrounding points exist for smoothing operation.\n",
    "    for coords in points_coords:\n",
    "        if coords[0] == 180:\n",
    "            coords[0] -= 1\n",
    "        elif coords[0] == -180:\n",
    "            coords[0] += 1\n",
    "        if coords[1] == 90:\n",
    "            coords[1] -= 1\n",
    "        elif coords[1] == -90:\n",
    "            coords[1] += 1\n",
    "\n",
    "    return points_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_term_average(ds, resample=\"D\", group_by_time_format='%m-%d'):\n",
    "    resampled_ds = ds.resample(time=resample).mean()\n",
    "    index_array = xr.DataArray(resampled_ds.indexes['time'].strftime(group_by_time_format), coords=resampled_ds.coords, name='time')\n",
    "    long_term_ave = resampled_ds.groupby(index_array).mean()\n",
    "    \n",
    "    return long_term_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_term_variance(ds, group_by_time_format='%m'):\n",
    "    index_array = xr.DataArray(ds.indexes['time'].strftime(group_by_time_format), coords=ds.coords, name='time')\n",
    "    long_term_var = ds.groupby(index_array).var()\n",
    "    \n",
    "    return long_term_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance_anomalies(time_series, resample='M', group_by_time_format='%m'):\n",
    "    period_variance = time_series.resample(time=resample).var()\n",
    "    long_term_var = long_term_variance(time_series, group_by_time_format=group_by_time_format)\n",
    "    index_array = xr.DataArray(period_variance.indexes['time'].strftime(group_by_time_format), coords=period_variance.coords, name='time')\n",
    "    var_anoms = period_variance.groupby(index_array) - long_term_var\n",
    "\n",
    "    return var_anoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth9(datum, ds, grid_res=1.0, timestamp='1979-01-01-00', resample=\"D\", group_by_time_format='%m-%d'):\n",
    "    # Get correct format for selecting by time index for long-term average dataset.\n",
    "    groupby_time = datetime.datetime.strptime(timestamp,\"%Y-%m-%d-%H\").strftime(group_by_time_format)\n",
    "\n",
    "    # Create variables for data and surrounding points using longitude and latitude.\n",
    "    lon = np.float64(datum[0])\n",
    "    lat = np.float64(datum[1])\n",
    "    lons = [lon - grid_res, lon, lon + grid_res]\n",
    "    lats = [lat - grid_res, lat, lat + grid_res]\n",
    "    \n",
    "    surrounding_points = np.array(np.meshgrid(lons, lats)).T.reshape(-1, 2)\n",
    "    surrounding_values = np.zeros((3,3))\n",
    "    weights = np.array([\n",
    "        [0.3, 0.5, 0.3],\n",
    "        [0.5, 1.0, 0.5],\n",
    "        [0.3, 0.5, 0.3]\n",
    "    ])\n",
    "    \n",
    "    # Get long-term average values for all points.\n",
    "    for i in range(len(surrounding_points)):\n",
    "        time_series = ds.sel(lon=surrounding_points[i][0], lat=surrounding_points[i][1])\n",
    "        long_term_ave = long_term_average(time_series, resample, group_by_time_format)\n",
    "        index = np.unravel_index(i, (3,3))\n",
    "        surrounding_values[index] = np.float64(long_term_ave.sel(time=groupby_time).to_array())\n",
    "    \n",
    "    # Calculate the smoothed value, sum of all weighted values divided by total weight.\n",
    "    weighted_values = np.multiply(surrounding_values, weights)\n",
    "    smoothed_value = np.sum(weighted_values)/np.sum(weights)\n",
    "    \n",
    "    return smoothed_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deseasonalise(datum, ds, timestamp='1979-01-01-00', grid_res=1.0, resample=\"D\", group_by_time_format='%m-%d'):\n",
    "    smoothed_long_term_ave = smooth9(datum, ds, grid_res, timestamp=timestamp, resample=resample, group_by_time_format=group_by_time_format)\n",
    "    datum_val = ds.sel(lon=datum[0], lat=datum[1], time=timestamp).to_array()\n",
    "    deseasonalised = np.float64(datum_val) - smoothed_long_term_ave\n",
    "    \n",
    "    return deseasonalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_vals(cdo_ds, calc_vals):\n",
    "    errors = 0\n",
    "    for key,calc_val in calc_vals.items():\n",
    "        print(key)\n",
    "        cdo_val = cdo_ds.sel(time=key[1], lon=key[0][0], lat=key[0][1])['z']\n",
    "        cdo_val = np.float64(cdo_val)\n",
    "        cdo_val_approx = np.round(cdo_val, 2)\n",
    "        calc_val_approx = np.round(calc_val, 2)\n",
    "        if pytest.approx(cdo_val_approx) != pytest.approx(calc_val_approx):\n",
    "            print('Difference between CDO and calculated values for: ' + str(key) + \n",
    "                  '\\n the two values are ' + str(cdo_val) + ' and ' + str(calc_val))\n",
    "            errors += 1\n",
    "    if errors == 0:\n",
    "        print('No discrepancies found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'G:\\\\Isaac\\\\Documents\\\\msc-research\\\\data\\\\ERA5\\\\daily_data\\\\era5_h500_daily_1979_2021_1deg.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\msc-research\\lib\\site-packages\\xarray\\backends\\file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\msc-research\\lib\\site-packages\\xarray\\backends\\lru_cache.py:53\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m---> 53\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('G:\\\\Isaac\\\\Documents\\\\msc-research\\\\data\\\\ERA5\\\\daily_data\\\\era5_h500_daily_1979_2021_1deg.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False))]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m raw_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mera5_h500_daily_1979_2021_1deg.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m cdo_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mera5_h500_daily_1979_2021_1deg_deseasonalised.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mraw_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m cdo_ds \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(path \u001b[38;5;241m+\u001b[39m cdo_file)\n\u001b[0;32m     10\u001b[0m points_coords \u001b[38;5;241m=\u001b[39m choose_random_data(ds)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\msc-research\\lib\\site-packages\\xarray\\backends\\api.py:495\u001b[0m, in \u001b[0;36mopen_dataset\u001b[1;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, backend_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[0;32m    484\u001b[0m     decode_cf,\n\u001b[0;32m    485\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    491\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[0;32m    492\u001b[0m )\n\u001b[0;32m    494\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 495\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mopen_dataset(\n\u001b[0;32m    496\u001b[0m     filename_or_obj,\n\u001b[0;32m    497\u001b[0m     drop_variables\u001b[38;5;241m=\u001b[39mdrop_variables,\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoders,\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    500\u001b[0m )\n\u001b[0;32m    501\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[0;32m    502\u001b[0m     backend_ds,\n\u001b[0;32m    503\u001b[0m     filename_or_obj,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    511\u001b[0m )\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\msc-research\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:553\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[1;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    534\u001b[0m     filename_or_obj,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    549\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    550\u001b[0m ):\n\u001b[0;32m    552\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[1;32m--> 553\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    565\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\msc-research\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:382\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[1;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[0;32m    376\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    377\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    379\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[0;32m    380\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[0;32m    381\u001b[0m )\n\u001b[1;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\msc-research\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:330\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[1;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\msc-research\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:391\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\msc-research\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:385\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 385\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[0;32m    386\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\msc-research\\lib\\contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\msc-research\\lib\\site-packages\\xarray\\backends\\file_manager.py:187\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\msc-research\\lib\\site-packages\\xarray\\backends\\file_manager.py:205\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    203\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    204\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[1;32m--> 205\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opener(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32msrc\\netCDF4\\_netCDF4.pyx:2307\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\netCDF4\\_netCDF4.pyx:1925\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'G:\\\\Isaac\\\\Documents\\\\msc-research\\\\data\\\\ERA5\\\\daily_data\\\\era5_h500_daily_1979_2021_1deg.nc'"
     ]
    }
   ],
   "source": [
    "# Check pre-processing steps, up to deseasonalisation, but excluding bandpass filter.\n",
    "\n",
    "path = \"G:\\\\Isaac\\\\Documents\\\\msc-research\\\\data\\\\ERA5\\\\daily_data\\\\\"\n",
    "raw_file = \"era5_h500_daily_1979_2021_1deg.nc\"\n",
    "cdo_file = \"era5_h500_daily_1979_2021_1deg_deseasonalised.nc\"\n",
    "\n",
    "ds = xr.open_dataset(path + raw_file)\n",
    "cdo_ds = xr.open_dataset(path + cdo_file)\n",
    "\n",
    "points_coords = choose_random_data(ds)\n",
    "print(points_coords)\n",
    "\n",
    "deseasonalised_data = {}\n",
    "timestamps = ['1979-01-01-00', '2000-06-06-12']\n",
    "\n",
    "for timestamp in timestamps:\n",
    "    for i,datum in enumerate(points_coords):\n",
    "        deseasonalised_datum = deseasonalise(datum, ds, timestamp=timestamp, grid_res=1.0)\n",
    "        deseasonalised_data[tuple(datum), timestamp] = deseasonalised_datum\n",
    "        print(str(i) + ': Done')\n",
    "\n",
    "errors = compare_vals(cdo_ds, deseasonalised_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[178  36]\n",
      " [ 21  -5]\n",
      " [177  15]\n",
      " [-77  77]\n",
      " [-74  69]]\n",
      "(178, 36) 1979-01-31\n",
      "(21, -5) 1979-01-31\n",
      "(177, 15) 1979-01-31\n",
      "(-77, 77) 1979-01-31\n",
      "(-74, 69) 1979-01-31\n",
      "(178, 36) 2000-06-30\n",
      "(21, -5) 2000-06-30\n",
      "(177, 15) 2000-06-30\n",
      "(-77, 77) 2000-06-30\n",
      "(-74, 69) 2000-06-30\n",
      "((178, 36), '1979-01-31')\n",
      "((21, -5), '1979-01-31')\n",
      "((177, 15), '1979-01-31')\n",
      "((-77, 77), '1979-01-31')\n",
      "((-74, 69), '1979-01-31')\n",
      "((178, 36), '2000-06-30')\n",
      "((21, -5), '2000-06-30')\n",
      "((177, 15), '2000-06-30')\n",
      "((-77, 77), '2000-06-30')\n",
      "((-74, 69), '2000-06-30')\n",
      "No discrepancies found.\n"
     ]
    }
   ],
   "source": [
    "# Check calculations of monthly variance anomalies.\n",
    "\n",
    "path = \"G:\\\\Isaac\\\\Documents\\\\msc-research\\\\data\\\\ERA5\\\\daily_data\\\\\"\n",
    "raw_file = \"era5_h500_daily_1979_2021_1deg_deseasonalised_bandpass.nc\"\n",
    "cdo_file = \"era5_h500_daily_1979_2021_1deg_deseasonalised_bandpass_mon_var_anoms.nc\"\n",
    "\n",
    "ds = xr.open_dataset(path + raw_file)\n",
    "cdo_ds = xr.open_dataset(path + cdo_file)\n",
    "\n",
    "points_coords = choose_random_data(ds)\n",
    "print(points_coords)\n",
    "\n",
    "monthly_variance_anomalies = {}\n",
    "timestamps = ['1979-01-31', '2000-06-30']\n",
    "\n",
    "for timestamp in timestamps:\n",
    "    for i,datum in enumerate(points_coords):\n",
    "        time_series = ds.isel(lon=datum[0], lat=datum[1])\n",
    "        var_anoms = calculate_variance_anomalies(time_series)\n",
    "        var_anom = var_anoms.sel(time=timestamp).to_array()\n",
    "        print(tuple(datum), timestamp)\n",
    "        monthly_variance_anomalies[tuple(datum), timestamp] = np.float64(var_anom)\n",
    "\n",
    "errors = compare_vals(cdo_ds, monthly_variance_anomalies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
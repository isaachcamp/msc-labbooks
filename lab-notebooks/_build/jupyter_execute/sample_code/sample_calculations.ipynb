{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Calculations\n",
    "\n",
    "Five random points at two different pre-selected timestamps are used to test whether the CDO software accurately calculated pre-processing steps as well as anomalies, means and variances.\n",
    "\n",
    "The bandpass calculation was not recreated as the process is computationally expensive -- CDO took over two days of CPU time, even with various optimisation procedures.\n",
    "\n",
    "Deseasonlisation and long-term averages show no discrepancies between the sample calculations and CDO. Neither do the variance and mean height anomalies -- use of CDO module is considered valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_data(ds):\n",
    "    arr_size = ds.lat.size * ds.lon.size\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    points = rng.choice(arr_size, 5)\n",
    "\n",
    "    points_indices = np.array(np.unravel_index(points, (ds.lon.size, ds.lat.size))).transpose()\n",
    "    points_coords = points_indices - np.array([180, 90])\n",
    "\n",
    "    # Ensure all surrounding points exist for smoothing operation.\n",
    "    for coords in points_coords:\n",
    "        if coords[0] == 180:\n",
    "            coords[0] -= 1\n",
    "        elif coords[0] == -180:\n",
    "            coords[0] += 1\n",
    "        if coords[1] == 90:\n",
    "            coords[1] -= 1\n",
    "        elif coords[1] == -90:\n",
    "            coords[1] += 1\n",
    "\n",
    "    return points_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_term_average(ds, resample=\"D\", group_by_time_format='%m-%d'):\n",
    "    resampled_ds = ds.resample(time=resample).mean()\n",
    "    index_array = xr.DataArray(resampled_ds.indexes['time'].strftime(group_by_time_format), coords=resampled_ds.coords, name='time')\n",
    "    long_term_ave = resampled_ds.groupby(index_array).mean()\n",
    "    \n",
    "    return long_term_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_term_variance(ds, group_by_time_format='%m'):\n",
    "    index_array = xr.DataArray(ds.indexes['time'].strftime(group_by_time_format), coords=ds.coords, name='time')\n",
    "    long_term_var = ds.groupby(index_array).var()\n",
    "    \n",
    "    return long_term_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance_anomalies(time_series, resample='M', group_by_time_format='%m'):\n",
    "    period_variance = time_series.resample(time=resample).var()\n",
    "    long_term_var = long_term_variance(time_series, group_by_time_format=group_by_time_format)\n",
    "    index_array = xr.DataArray(period_variance.indexes['time'].strftime(group_by_time_format), coords=period_variance.coords, name='time')\n",
    "    var_anoms = period_variance.groupby(index_array) - long_term_var\n",
    "\n",
    "    return var_anoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth9(datum, ds, grid_res=1.0, timestamp='1979-01-01-00', resample=\"D\", group_by_time_format='%m-%d'):\n",
    "    # Get correct format for selecting by time index for long-term average dataset.\n",
    "    groupby_time = datetime.datetime.strptime(timestamp,\"%Y-%m-%d-%H\").strftime(group_by_time_format)\n",
    "\n",
    "    # Create variables for data and surrounding points using longitude and latitude.\n",
    "    lon = np.float64(datum[0])\n",
    "    lat = np.float64(datum[1])\n",
    "    lons = [lon - grid_res, lon, lon + grid_res]\n",
    "    lats = [lat - grid_res, lat, lat + grid_res]\n",
    "    \n",
    "    surrounding_points = np.array(np.meshgrid(lons, lats)).T.reshape(-1, 2)\n",
    "    surrounding_values = np.zeros((3,3))\n",
    "    weights = np.array([\n",
    "        [0.3, 0.5, 0.3],\n",
    "        [0.5, 1.0, 0.5],\n",
    "        [0.3, 0.5, 0.3]\n",
    "    ])\n",
    "    \n",
    "    # Get long-term average values for all points.\n",
    "    for i in range(len(surrounding_points)):\n",
    "        time_series = ds.sel(lon=surrounding_points[i][0], lat=surrounding_points[i][1])\n",
    "        long_term_ave = long_term_average(time_series, resample, group_by_time_format)\n",
    "        index = np.unravel_index(i, (3,3))\n",
    "        surrounding_values[index] = np.float64(long_term_ave.sel(time=groupby_time).to_array())\n",
    "    \n",
    "    # Calculate the smoothed value, sum of all weighted values divided by total weight.\n",
    "    weighted_values = np.multiply(surrounding_values, weights)\n",
    "    smoothed_value = np.sum(weighted_values)/np.sum(weights)\n",
    "    \n",
    "    return smoothed_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deseasonalise(datum, ds, timestamp='1979-01-01-00', grid_res=1.0, resample=\"D\", group_by_time_format='%m-%d'):\n",
    "    smoothed_long_term_ave = smooth9(datum, ds, grid_res, timestamp=timestamp, resample=resample, group_by_time_format=group_by_time_format)\n",
    "    datum_val = ds.sel(lon=datum[0], lat=datum[1], time=timestamp).to_array()\n",
    "    deseasonalised = np.float64(datum_val) - smoothed_long_term_ave\n",
    "    \n",
    "    return deseasonalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_vals(cdo_ds, calc_vals):\n",
    "    errors = 0\n",
    "    for key,calc_val in calc_vals.items():\n",
    "        print(key)\n",
    "        cdo_val = cdo_ds.sel(time=key[1], lon=key[0][0], lat=key[0][1])['z']\n",
    "        cdo_val = np.float64(cdo_val)\n",
    "        cdo_val_approx = np.round(cdo_val, 2)\n",
    "        calc_val_approx = np.round(calc_val, 2)\n",
    "        if pytest.approx(cdo_val_approx) != pytest.approx(calc_val_approx):\n",
    "            print('Difference between CDO and calculated values for: ' + str(key) + \n",
    "                  '\\n the two values are ' + str(cdo_val) + ' and ' + str(calc_val))\n",
    "            errors += 1\n",
    "    if errors == 0:\n",
    "        print('No discrepancies found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pre-processing steps, up to deseasonalisation, but excluding bandpass filter.\n",
    "\n",
    "path = \"G:\\\\Isaac\\\\Documents\\\\msc-research\\\\data\\\\ERA5\\\\daily_data\\\\\"\n",
    "raw_file = \"era5_h500_daily_1979_2021_1deg.nc\"\n",
    "cdo_file = \"era5_h500_daily_1979_2021_1deg_deseasonalised.nc\"\n",
    "\n",
    "ds = xr.open_dataset(path + raw_file)\n",
    "cdo_ds = xr.open_dataset(path + cdo_file)\n",
    "\n",
    "points_coords = choose_random_data(ds)\n",
    "print(points_coords)\n",
    "\n",
    "deseasonalised_data = {}\n",
    "timestamps = ['1979-01-01-00', '2000-06-06-12']\n",
    "\n",
    "for timestamp in timestamps:\n",
    "    for i,datum in enumerate(points_coords):\n",
    "        deseasonalised_datum = deseasonalise(datum, ds, timestamp=timestamp, grid_res=1.0)\n",
    "        deseasonalised_data[tuple(datum), timestamp] = deseasonalised_datum\n",
    "        print(str(i) + ': Done')\n",
    "\n",
    "errors = compare_vals(cdo_ds, deseasonalised_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[178  36]\n",
      " [ 21  -5]\n",
      " [177  15]\n",
      " [-77  77]\n",
      " [-74  69]]\n",
      "(178, 36) 1979-01-31\n",
      "(21, -5) 1979-01-31\n",
      "(177, 15) 1979-01-31\n",
      "(-77, 77) 1979-01-31\n",
      "(-74, 69) 1979-01-31\n",
      "(178, 36) 2000-06-30\n",
      "(21, -5) 2000-06-30\n",
      "(177, 15) 2000-06-30\n",
      "(-77, 77) 2000-06-30\n",
      "(-74, 69) 2000-06-30\n",
      "((178, 36), '1979-01-31')\n",
      "((21, -5), '1979-01-31')\n",
      "((177, 15), '1979-01-31')\n",
      "((-77, 77), '1979-01-31')\n",
      "((-74, 69), '1979-01-31')\n",
      "((178, 36), '2000-06-30')\n",
      "((21, -5), '2000-06-30')\n",
      "((177, 15), '2000-06-30')\n",
      "((-77, 77), '2000-06-30')\n",
      "((-74, 69), '2000-06-30')\n",
      "No discrepancies found.\n"
     ]
    }
   ],
   "source": [
    "# Check calculations of monthly variance anomalies.\n",
    "\n",
    "path = \"G:\\\\Isaac\\\\Documents\\\\msc-research\\\\data\\\\ERA5\\\\daily_data\\\\\"\n",
    "raw_file = \"era5_h500_daily_1979_2021_1deg_deseasonalised_bandpass.nc\"\n",
    "cdo_file = \"era5_h500_daily_1979_2021_1deg_deseasonalised_bandpass_mon_var_anoms.nc\"\n",
    "\n",
    "ds = xr.open_dataset(path + raw_file)\n",
    "cdo_ds = xr.open_dataset(path + cdo_file)\n",
    "\n",
    "points_coords = choose_random_data(ds)\n",
    "print(points_coords)\n",
    "\n",
    "monthly_variance_anomalies = {}\n",
    "timestamps = ['1979-01-31', '2000-06-30']\n",
    "\n",
    "for timestamp in timestamps:\n",
    "    for i,datum in enumerate(points_coords):\n",
    "        time_series = ds.isel(lon=datum[0], lat=datum[1])\n",
    "        var_anoms = calculate_variance_anomalies(time_series)\n",
    "        var_anom = var_anoms.sel(time=timestamp).to_array()\n",
    "        print(tuple(datum), timestamp)\n",
    "        monthly_variance_anomalies[tuple(datum), timestamp] = np.float64(var_anom)\n",
    "\n",
    "errors = compare_vals(cdo_ds, monthly_variance_anomalies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
# Philosophy of Model Evaluation

## Climate Model Confirmation: From Philosophy to Predicting Climate in the Real World, Knutti 2018 {cite}`knutti_climate_2018`

Knutti argues the way to evaluate a model is through understanding how it might illuminate physical processes. He argues, "for the particular purpose of interest, (1) the relevant quantitative relationships or interaction between different parts or variables that emerge from the inner structure of the model are sufficiently similar to those in the target system, (2) they will remain so over time and beyond the range where data is available for evaluation, and (3) no important part or interaction, either known or unknown, is missing." Models should be used to infer something about the physical world, not to be completely representative, as this is impossible.

From this argument, we can see that the no model will necessarily fulfil the fitness-for-purpose criteria, given that many of the physical processes associated with storm tracks are no well-understood and there is little consensus on how several general characteristics arise, see Lorenz & Hartmann (2001){cite}`lorenz_eddy-zonal_2001`, Kidston et al. (2010){cite}`kidston_observations_2010`, (ask James for the contradictory mechanism published after Kidston). 

He breaks uncertainties down into five categories, splitting model uncertainties {cite}`gettelman_demystifying_2016` into three sub-categories: 
model structure; resolution; and parameterisation. Model structure could be the intentional neglect of explicit ice sheet description or ocean dynamics. These three are the core representational uncertainties in modelling the climate, which are solely belonging to model composition.

Using an MME, if there is a "common core (causal) structure" identified, it is likely to have a corresponding structure in the physical world. However, this reasoning cannot be applied without caveats, as there are universal biases in models that are known to be incorrect, such as the ITCZ shape and location and the equatorward mean location of the SH storm track {cite}`kidston_intermodel_2010`. An MME might be used as part of a wider body of evidence supporting one line of reasoning {cite}`knutti_climate_2018`{cite}`parker_model_2020`, as is frequently done in detection and attribution studies -- _find citations, check TAR ch.12 and AR5 ch.10_ {cite}``

He suggests that model complexity does not determine usefulness, as simplified models can be better suited to illuminating a certain process given the simpler dissection and attribution of causes. 

## Verification, Validation, and Confirmation of Numerical Models in the Earth Sciences, Oreskes et al. 1994 {cite}`oreskes_verification_1994`

"What we call data are inference-laden signifiers of natural phenomena to which we have incomplete access" -- Yummy.

The authors assert that models can never be truly verified -- asserting the "truth" of a model -- as they are not a closed system. A closed system is one which can be fully defined mathematically. Models are composed of assumptions and parameterisations; they are incomplete representations of physical processes, leaving the system open. This necessarily leaves realisations of a model unverifiable.

Models output non-unique solutions, meaning models can be constructed in numerous ways and still produce the same answer. This may lead one to commit the logical fallacy of "affirming the consequent", asserting that a model is verified because it closely matches observations. However, this close match may be for any number of reasons, including the possibility that two erroneous equations cancel each other out in the short-term -- over the time period of a historical simulation -- but account for significant discrepancies once extrapolated. This may lead one to conclude models are therefore of little use, however, a close match between observations and a realisation of a model supports the fidelity of a model. This is also where the use of multi-model ensembles can provide confidence in projections, under the assumption of independence.

The authors encourage the use of neutral language that discourages the either-or line of reasoning where a model is either valid and useful or it isn't. Models can be useful, with the prior knowledge that the results do not provide truth, rather they provide evidence that can support or deny a hypothesis, and that a mounted weight of evidence is whats required for a hypothesis to be accepted as the _most likely_ theory.


## Model Evaluation: An Adequacy-for-Purpose View, Parker 2020 {cite}`parker_model_2020`

Parker argues that we though we must accept models are inherently incorrect, they can and should be evaluated under the basis of being fit to answer questions of interest. They define the terms "adequacy-for-purpose" and "fit-for-purpose", the former being (roughly) whether a model achieves its intended purpose, the latter whether it achieves its purpose to a given standard.

Parker offers a few basic strategies for assessing model adequacy-for-purpose, of use to us is the model construction and component performance, where the components are made up of models that feasibly represent processes required for evaluation -- in our case, atmsopheric circulation and dynamics. From our perspective as a model evaluator, considerations outside of simply representational accuracy must be accounted for, such as model construction, the user or the methodology applied, when assessing adequacy-for-purpose.


## Ensemble modeling, uncertainty and robust predictions, Parker 2013 {cite}`parker_ensemble_2013`

Parker asserts that using different types of ensemble, such as multi-member (MME) and perturbed-physics (PPE), doesn't preclude the possibility of structural uncertainty -- all contributing members possess the same biases that fail to capture, for example, a key driver of climate change. She argues the significance of robust projections is hard to gauge given this flaw. This lends itself as a justification for diagnosing biases.
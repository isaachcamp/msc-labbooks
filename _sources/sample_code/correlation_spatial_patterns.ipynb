{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Spatial Patterns\n",
    "\n",
    "This notebook calculates the correlation between indices time series and anomaly time series, variance and mean height, at each longitude/latitude grid point. Methods used are both Pearson and Spearman rank correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import glob\n",
    "\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(variance_anoms):\n",
    "    # Creates 2D array of coordinate pairs\n",
    "    lon_min = np.int32(np.array(variance_anoms.lon)[0])\n",
    "    lon_max = np.int32(np.array(variance_anoms.lon)[-1])\n",
    "    lat_min = np.int32(np.array(variance_anoms.lat)[0])\n",
    "    lat_max = np.int32(np.array(variance_anoms.lat)[-1])\n",
    "\n",
    "    lon = [*range(lon_min, lon_max+1)]\n",
    "    lat = [*range(lat_min, lat_max-1, -1)]\n",
    "\n",
    "    coords_array = np.array(np.meshgrid(lon, lat)).T.reshape(-1,2)\n",
    "\n",
    "    return coords_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlation_dict(variance_anoms, index_dict):   \n",
    "    correlation_arrays = {}\n",
    "    \n",
    "    shape = (variance_anoms.lon.size, variance_anoms.lat.size)\n",
    "    corr_array = np.zeros(shape)\n",
    "\n",
    "    for key in index_dict.keys():\n",
    "        correlation_arrays[key] = np.copy(corr_array)\n",
    "\n",
    "    return correlation_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_sample(arr):\n",
    "    if np.round(np.mean(arr),6) != pytest.approx(0.0):\n",
    "        arr_anoms = arr - np.mean(arr)\n",
    "    else:\n",
    "        arr_anoms = arr\n",
    "    if np.std(arr_anoms) != pytest.approx(1.0):\n",
    "        arr_norm = np.divide(arr_anoms, np.std(arr_anoms))\n",
    "    else:\n",
    "        arr_norm = arr_anoms\n",
    "    \n",
    "    return arr_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_two_normalised_time_series(arr1, arr2):\n",
    "    return np.sum(np.multiply(arr1, arr2)) / (arr1.size - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_time_series(variance_anoms, coords):\n",
    "    variance_anoms_dbl_array = np.array(variance_anoms.sel(lon=coords[0], lat=coords[1]).to_array()[1])\n",
    "    variance_anoms_array = np.hsplit(variance_anoms_dbl_array, 2)[0]\n",
    "    variance_time_series = variance_anoms_array.flatten()\n",
    "\n",
    "    return variance_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indices(filenames, variable_names):\n",
    "    index_dict = {}\n",
    "    for filename in filenames:\n",
    "        raw_index = xr.open_dataset(filename)\n",
    "        for variable_name in list(raw_index.keys()):\n",
    "            if variable_name in variable_names:\n",
    "                index_dict[variable_name] = np.array(raw_index[variable_name]).flatten()\n",
    "                # Drop nans\n",
    "                index_dict[variable_name] = index_dict[variable_name][~np.isnan(index_dict[variable_name])]\n",
    "                index_dict[variable_name] = normalise_sample(index_dict[variable_name])\n",
    "\n",
    "    return index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation_matrices(anoms, index_dict, coords_array, correlation_arrays, method='pearson'):\n",
    "    for i,coord in enumerate(coords_array):\n",
    "        time_series = reformat_time_series(anoms, coords=coord)\n",
    "        time_series_normalised = normalise_sample(time_series)\n",
    "        for key,index in index_dict.items():\n",
    "            resized_time_series_normalised = time_series_normalised[:index.size]\n",
    "            if method == 'pearson':\n",
    "                correlation = correlation_two_normalised_time_series(index, resized_time_series_normalised)\n",
    "            elif method == 'rank':\n",
    "                correlation, pval = spearmanr(resized_time_series_normalised, index)\n",
    "            coord = np.unravel_index(i,correlation_arrays[key].shape)\n",
    "            correlation_arrays[key][coord] = correlation\n",
    "\n",
    "    return correlation_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_correlations_netcdf(correlation_arrays, anoms, var='variance', method='pearson'):\n",
    "    corr_path = path + \"correlations//\"\n",
    "    for key in correlation_arrays.keys():\n",
    "        correlations_ds = xr.Dataset(\n",
    "            data_vars={key:(('lat', 'lon'), correlation_arrays[key])},\n",
    "            coords={'lat':anoms.lat, 'lon':anoms.lon}\n",
    "        )\n",
    "        filename = var + '_' + key + '_' + method + '_correlations.nc'\n",
    "        correlations_ds.to_netcdf(corr_path + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"G:\\\\Isaac\\\\Documents\\\\msc-research\\\\data\\\\ERA5\\\\daily_data\\\\\"\n",
    "var_file = \"era5_h500_daily_1979_2021_1deg_20S_deseasonalised_bandpass_mon_var_anoms.nc\"\n",
    "mean_file = \"era5_h500_daily_1979_2021_1deg_deseasonalised_bandpass_20S_mon_anoms.nc\"\n",
    "\n",
    "index_path = \"G:\\\\Isaac\\\\Documents\\\\msc-research\\\\data\\\\indices\\\\\"\n",
    "index_files = glob.glob(index_path + \"*.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_anoms = xr.open_dataset(path + var_file)\n",
    "mean_anoms = xr.open_dataset(path + mean_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = ['z', 'DMI', 'SOI']\n",
    "index_dict = load_indices(index_files, variable_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_array = get_coords(variance_anoms)\n",
    "correlation_arrays = create_correlation_dict(variance_anoms, index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_pearson_correlation_arrays = calculate_correlation_matrices(variance_anoms, index_dict, coords_array, correlation_arrays, 'pearson')\n",
    "variance_rank_correlation_arrays = calculate_correlation_matrices(variance_anoms, index_dict, coords_array, correlation_arrays, 'rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pearson_correlation_arrays = calculate_correlation_matrices(mean_anoms, index_dict, coords_array, correlation_arrays, 'pearson')\n",
    "mean_rank_correlation_arrays = calculate_correlation_matrices(mean_anoms, index_dict, coords_array, correlation_arrays, 'rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_correlations_netcdf(variance_pearson_correlation_arrays, mean_anoms, 'variance', method='pearson')\n",
    "write_correlations_netcdf(variance_rank_correlation_arrays, mean_anoms, 'variance', method='rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_correlations_netcdf(mean_pearson_correlation_arrays, mean_anoms, 'mean', method='pearson')\n",
    "write_correlations_netcdf(mean_rank_correlation_arrays, mean_anoms, 'mean', method='rank')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
